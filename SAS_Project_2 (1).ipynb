{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLOpaher1tsQ"
      },
      "source": [
        "\n",
        "# Fishman Maxim "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_3IgBs51tsR",
        "outputId": "bb62013e-a602-45ad-f80c-51be16fdb61e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting researchpy\n",
            "  Downloading researchpy-0.3.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from researchpy) (0.10.2)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.7/dist-packages (from researchpy) (0.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from researchpy) (1.21.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from researchpy) (1.3.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from researchpy) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->researchpy) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->researchpy) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->researchpy) (1.15.0)\n",
            "Installing collected packages: researchpy\n",
            "Successfully installed researchpy-0.3.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "!pip install researchpy\n",
        "import researchpy as rp\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from scipy import stats\n",
        "from scipy.interpolate import *\n",
        "from matplotlib.pyplot import *\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "import matplotlib.pylab as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import statistics\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUM2hYH91tsR",
        "outputId": "f8469418-abc5-4c61-eee7-001ed7cc892c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2fecad0c71a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_accept\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"13_accept.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_reject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"13_reject.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_accept\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'UID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_reject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'UID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '13_accept.csv'"
          ]
        }
      ],
      "source": [
        "df_accept = pd.read_csv(\"13_accept.csv\")\n",
        "df_reject = pd.read_csv(\"13_reject.csv\")\n",
        "df_accept.drop('UID', axis=1, inplace=True)\n",
        "df_reject.drop('UID', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WejFHAIC1tsS"
      },
      "source": [
        "## 1. Какая доля 1 в выборке \"accept\"?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-gUAocm1tsS"
      },
      "outputs": [],
      "source": [
        "print(\"Доля 1:\", df_accept['target'].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn4nhEwb1tsT"
      },
      "source": [
        "## 2. Необходимо рассчитать для всех интервальных переменных следующее:\n",
        "  - Доля пропущенных значений \n",
        "  - Медиана\n",
        "  - Среднее\n",
        "  - Среднеквадратическое отклонение\n",
        "  - Есть ли аномальные значения, выбросы? \n",
        "  - Information Value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT40CPVb1tsT"
      },
      "source": [
        "### 2.1 Accept"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMePuEuS1tsT"
      },
      "outputs": [],
      "source": [
        "df_accept.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7vkspMy1tsU"
      },
      "source": [
        "There are 8 interval variables:\n",
        "\n",
        "\"income\", \"COUNT_ACTIVE_CREDIT_NO_CC\", \"CNT_MNTH_FROM_LAST_PMNT\", \"age\", \"experience\", \"inquiry_14_day\", \"inquiry_21_day\", \"count_mnth_act_passport\"\n",
        "\n",
        "(After getting rid of Nan values I saw see that \"DEPENDANT_NUMBER\" variable has only 5 possible values => I will use it as categorical variable.) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXLD3iUw1tsU"
      },
      "source": [
        "I will replace Nans with medians for discreet variables and with means for continuous variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rc-Q-wUF1tsU"
      },
      "outputs": [],
      "source": [
        "int_accept1 = pd.DataFrame(df_accept,columns=['income','COUNT_ACTIVE_CREDIT_NO_CC',\n",
        "                                             'CNT_MNTH_FROM_LAST_PMNT','age', 'experience', \n",
        "                                             'inquiry_14_day','inquiry_21_day','count_mnth_act_passport'])\n",
        "int_accept = int_accept1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFXWWpoW1tsU"
      },
      "outputs": [],
      "source": [
        "int_accept['CNT_MNTH_FROM_LAST_PMNT'] = int_accept['CNT_MNTH_FROM_LAST_PMNT'].fillna(int_accept[\"CNT_MNTH_FROM_LAST_PMNT\"].mean())\n",
        "int_accept['experience'] = int_accept['experience'].fillna(int_accept[\"experience\"].mean())\n",
        "int_accept['count_mnth_act_passport'] = int_accept['count_mnth_act_passport'].fillna(int_accept[\"count_mnth_act_passport\"].mean())\n",
        "int_accept['age'] = int_accept['age'].fillna(int_accept[\"age\"].mean())\n",
        "\n",
        "int_accept['income'] = int_accept['income'].fillna(int_accept[\"income\"].median())\n",
        "int_accept['COUNT_ACTIVE_CREDIT_NO_CC'] = int_accept['COUNT_ACTIVE_CREDIT_NO_CC'].fillna(int_accept[\"COUNT_ACTIVE_CREDIT_NO_CC\"].median())\n",
        "int_accept['inquiry_14_day'] = int_accept['inquiry_14_day'].fillna(int_accept[\"inquiry_14_day\"].median())\n",
        "int_accept['inquiry_21_day'] = int_accept['inquiry_21_day'].fillna(int_accept[\"inquiry_21_day\"].median())\n",
        "\n",
        "if(int_accept.isnull().values.any() == False):\n",
        "    print(\"No more Nans\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDAN7s4v1tsV"
      },
      "source": [
        "Here, I will calculate percentage of Nan variables, mean, median and standard deviation. To check anomalies in the data I will use z-score method and I will count the number of outliers for each variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB_IhHUz1tsV"
      },
      "outputs": [],
      "source": [
        "percent_non = []\n",
        "percent_exst = []\n",
        "count_none = []\n",
        "col_names = []\n",
        "for col in int_accept1:\n",
        "    col_names.append(col)\n",
        "    percent_non.append((df_accept[col].isnull().sum())/(len(df_accept)/100))\n",
        "    count_none.append(df_accept[col].isnull().sum())\n",
        "    \n",
        "\n",
        "median = []\n",
        "mean = []\n",
        "std = []\n",
        "outliers = []\n",
        "\n",
        "\n",
        "def z_score_method(df, variable_name):\n",
        "    columns = df.columns\n",
        "    z = np.abs(stats.zscore(df))\n",
        "    threshold = 3\n",
        "    outlier = []\n",
        "    index=0\n",
        "    for item in range(len(columns)):\n",
        "        if columns[item] == variable_name:\n",
        "            index = item\n",
        "    for i, v in enumerate(z[:, index]):\n",
        "        if v > threshold:\n",
        "            outlier.append(i)\n",
        "        else:\n",
        "            continue\n",
        "    return outlier\n",
        "    \n",
        "for col in int_accept:\n",
        "    median.append(int_accept[col].median())\n",
        "    mean.append(int_accept[col].mean())\n",
        "    std.append(int_accept[col].std())\n",
        "    outliers.append(len(z_score_method(int_accept, col)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBrJ62Qi1tsV"
      },
      "source": [
        "The following function calculates IV for each varible using WOE. The method of calculating WOE has two steps:\n",
        "\n",
        "1) Fine classing: I separated data in 20 bins and calculate WOE value for each bin;\n",
        "\n",
        "2) Coarse classing: In order to decrease number of bins I combine bins with close WOE values;\n",
        "\n",
        "Then the function calculates IV for each bin and sums up values to get IV for the variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBhIq0dh1tsV"
      },
      "outputs": [],
      "source": [
        "def get_IV(df, feature, target):\n",
        "    lst = []\n",
        "    bins = df[feature].quantile(np.linspace(0, 1, 21)).values\n",
        "    bin_index = range(1, len(bins))\n",
        "    for index in bin_index:\n",
        "        val = df[(df[feature] >= bins[index-1])&(df[feature] <= bins[index])]\n",
        "        lst.append([feature,               \n",
        "                    val[(val[target] == 0)].count()[feature],  # Good \n",
        "                    val[(val[target] == 1)].count()[feature]   # Bad  \n",
        "                   ])\n",
        " \n",
        "    data = pd.DataFrame(lst, columns=['Variable', 'Good', 'Bad'])\n",
        "\n",
        "        \n",
        "    total_bad = df[df[target] == 1].count()[feature]\n",
        "    total_good = df.shape[0] - total_bad\n",
        "    \n",
        "    data['Distribution Good'] = data['Good']/ total_good\n",
        "    data['Distribution Bad'] = data['Bad'] / total_bad\n",
        "    data['WoE'] = np.log(data['Distribution Good'] / data['Distribution Bad'])\n",
        "\n",
        "    data = data.replace({'WoE': {np.inf: 0, -np.inf: 0}})\n",
        "    data = data.sort_values(by=['WoE'], ascending=[True])\n",
        "    data.index = range(len(data.index))\n",
        "    \n",
        "    data1 = data\n",
        "    \n",
        "        \n",
        "    #Combine bins with similar WOE values\n",
        "    n = range(1, len(data1))\n",
        "    for i in n:\n",
        "        if (data1['WoE'][i] - data1['WoE'][i-1] < 0.08):\n",
        "            data.loc[i,'WoE'] = data1['WoE'][i-1]\n",
        "    \n",
        "    \n",
        "    #Find IV for variable\n",
        "    iv_all = []\n",
        "    uniq = data['WoE'].unique()\n",
        "    for val in uniq:\n",
        "        b = data[(data['WoE'] == val)]\n",
        "        b.index = range(len(b.index))\n",
        "        good = b['Good'].sum()\n",
        "        bad = b['Bad'].sum()\n",
        "        ival = val * (good/total_good - bad/total_bad)\n",
        "        iv_all.append(ival)\n",
        "        \n",
        "\n",
        "    iv = sum(iv_all)\n",
        "\n",
        "    return iv\n",
        "\n",
        "    \n",
        "int_accept2 = pd.DataFrame(df_accept,columns=['income','COUNT_ACTIVE_CREDIT_NO_CC',\n",
        "                                             'CNT_MNTH_FROM_LAST_PMNT','age', 'experience', \n",
        "                                             'inquiry_14_day','inquiry_21_day','count_mnth_act_passport','target'])\n",
        "IV = []\n",
        "\n",
        "IV.append(get_IV(int_accept2, 'income', 'target'))\n",
        "IV.append(get_IV(int_accept2, 'COUNT_ACTIVE_CREDIT_NO_CC', 'target'))\n",
        "IV.append(get_IV(int_accept2, 'CNT_MNTH_FROM_LAST_PMNT', 'target'))\n",
        "IV.append(get_IV(int_accept2, 'age', 'target'))\n",
        "IV.append(get_IV(int_accept2, 'experience', 'target'))\n",
        "IV.append(get_IV(int_accept2, 'inquiry_14_day', 'target'))\n",
        "IV.append(get_IV(int_accept2, 'inquiry_21_day', 'target'))\n",
        "IV.append(get_IV(int_accept2, 'count_mnth_act_passport', 'target'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYcOb4LS1tsW"
      },
      "source": [
        "The following table shows information for each interval variable in Accept.scv:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oa5YUYh1tsW"
      },
      "outputs": [],
      "source": [
        "data = {'Names': col_names,\n",
        "        'None_values%': percent_non,\n",
        "        'Median': median,\n",
        "        'Mean': mean,\n",
        "        'Std': std,\n",
        "        'Num_Outliers': outliers,\n",
        "        'IV': IV\n",
        "       } \n",
        "int_info = pd.DataFrame(data,columns=['Names','None_values%','Median','Mean','Std','Num_Outliers','IV'])\n",
        "int_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YchMK_nt1tsW"
      },
      "source": [
        "### 2.2 Reject"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5RPRDJE1tsW"
      },
      "source": [
        "Now, I will redo the same steps for Reject.csv but for now I cannot count IV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svWumV7g1tsW"
      },
      "outputs": [],
      "source": [
        "int_reject1 = pd.DataFrame(df_reject,columns=['income','COUNT_ACTIVE_CREDIT_NO_CC',\n",
        "                                             'CNT_MNTH_FROM_LAST_PMNT','age', 'experience', \n",
        "                                             'inquiry_14_day','inquiry_21_day','count_mnth_act_passport'])\n",
        "int_reject = int_reject1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rx7B9V5m1tsW"
      },
      "outputs": [],
      "source": [
        "int_reject['CNT_MNTH_FROM_LAST_PMNT'] = int_reject['CNT_MNTH_FROM_LAST_PMNT'].fillna(int_reject[\"CNT_MNTH_FROM_LAST_PMNT\"].mean())\n",
        "int_reject['experience'] = int_reject['experience'].fillna(int_reject[\"experience\"].mean())\n",
        "int_reject['count_mnth_act_passport'] = int_reject['count_mnth_act_passport'].fillna(int_reject[\"count_mnth_act_passport\"].mean())\n",
        "int_reject['age'] = int_reject['age'].fillna(int_reject[\"age\"].mean())\n",
        "\n",
        "int_reject['income'] = int_reject['income'].fillna(int_reject[\"income\"].median())\n",
        "int_reject['COUNT_ACTIVE_CREDIT_NO_CC'] = int_reject['COUNT_ACTIVE_CREDIT_NO_CC'].fillna(int_reject[\"COUNT_ACTIVE_CREDIT_NO_CC\"].median())\n",
        "int_reject['inquiry_14_day'] = int_reject['inquiry_14_day'].fillna(int_reject[\"inquiry_14_day\"].median())\n",
        "int_reject['inquiry_21_day'] = int_reject['inquiry_21_day'].fillna(int_reject[\"inquiry_21_day\"].median())\n",
        "\n",
        "if(int_reject.isnull().values.any() == False):\n",
        "    print(\"No more Nans\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0xC8jhY1tsW"
      },
      "outputs": [],
      "source": [
        "percent_non = []\n",
        "percent_exst = []\n",
        "count_none = []\n",
        "col_names = []\n",
        "for col in int_reject1:\n",
        "    col_names.append(col)\n",
        "    percent_non.append((df_reject[col].isnull().sum())/(len(df_reject)/100))\n",
        "    count_none.append(df_reject[col].isnull().sum())\n",
        "    \n",
        "\n",
        "median = []\n",
        "mean = []\n",
        "std = []\n",
        "outliers = []\n",
        "\n",
        "\n",
        "def z_score_method(df, variable_name):\n",
        "    columns = df.columns\n",
        "    z = np.abs(stats.zscore(df))\n",
        "    threshold = 3\n",
        "    outlier = []\n",
        "    index=0\n",
        "    for item in range(len(columns)):\n",
        "        if columns[item] == variable_name:\n",
        "            index = item\n",
        "    for i, v in enumerate(z[:, index]):\n",
        "        if v > threshold:\n",
        "            outlier.append(i)\n",
        "        else:\n",
        "            continue\n",
        "    return outlier\n",
        "    \n",
        "for col in int_reject:\n",
        "    median.append(int_reject[col].median())\n",
        "    mean.append(int_reject[col].mean())\n",
        "    std.append(int_reject[col].std())\n",
        "    outliers.append(len(z_score_method(int_reject, col)))\n",
        "    \n",
        "data = {'Names': col_names,\n",
        "        'None_values%': percent_non,\n",
        "        'Median': median,\n",
        "        'Mean': mean,\n",
        "        'Std': std,\n",
        "        'Num_Outliers': outliers\n",
        "       } \n",
        "int_info = pd.DataFrame(data,columns=['Names','None_values%','Median','Mean','Std','Num_Outliers'])\n",
        "int_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74xKwUQU1tsX"
      },
      "source": [
        "## 3. Необходимо рассчитать для всех категориальных переменных следующее:\n",
        "  - Мода\n",
        "  - Доля пропущенных значений\n",
        "  - Information Value\n",
        "  - Есть ли выбросы, аномальные значений"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSdpLo6a1tsX"
      },
      "source": [
        "### 3.1 Accept"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFpwhmtE1tsX"
      },
      "source": [
        "There are 5 categorical variables: \n",
        "\n",
        "'INCOME_TYPE','EDUCATION_','CUSTOMER_CATEGORY_','DEPENDANT_NUMBER', 'IS_EMPL'\n",
        "\n",
        "NaNs I will replace with the most popular category.\n",
        "\n",
        "To deal with categorical variables I will use Label Encoding for variables as categorical features are ordinal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRWhAq151tsX"
      },
      "outputs": [],
      "source": [
        "cat_accept1 = pd.DataFrame(df_accept,columns=['INCOME_TYPE','EDUCATION_','CUSTOMER_CATEGORY_',\n",
        "                                             'DEPENDANT_NUMBER', 'IS_EMPL','target'])\n",
        "\n",
        "cat_accept = cat_accept1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woICkdeG1tsX"
      },
      "outputs": [],
      "source": [
        "cat_accept['INCOME_TYPE'] = cat_accept['INCOME_TYPE'].fillna(cat_accept.mode()['INCOME_TYPE'][0])\n",
        "cat_accept['EDUCATION_'] = cat_accept['EDUCATION_'].fillna(cat_accept.mode()['EDUCATION_'][0])\n",
        "cat_accept['CUSTOMER_CATEGORY_'] = cat_accept['CUSTOMER_CATEGORY_'].fillna(cat_accept.mode()['CUSTOMER_CATEGORY_'][0])\n",
        "cat_accept['DEPENDANT_NUMBER'] = cat_accept['DEPENDANT_NUMBER'].fillna(cat_accept.mode()['DEPENDANT_NUMBER'][0])\n",
        "cat_accept['IS_EMPL'] = cat_accept['IS_EMPL'].fillna(cat_accept.mode()['IS_EMPL'][0])\n",
        "\n",
        "if(cat_accept.isnull().values.any() == False):\n",
        "    print(\"No more Nans\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EzFnyS01tsX"
      },
      "outputs": [],
      "source": [
        "percent_non = []\n",
        "col_names = []\n",
        "\n",
        "\n",
        "for col in cat_accept1:\n",
        "    col_names.append(col)\n",
        "    percent_non.append((df_accept[col].isnull().sum())/(len(df_accept)/100))\n",
        "    \n",
        "\n",
        "mode = []\n",
        "\n",
        "for col in cat_accept:\n",
        "    mode.append(cat_accept.mode()[col][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqCX86K11tsX"
      },
      "outputs": [],
      "source": [
        "cat_accept2 = cat_accept\n",
        "cat_accept[\"EDUCATION_\"].replace({\"beginner\": 0, \"elementary\": 1, \"high\": 2, \"advanced\": 3}, inplace=True)\n",
        "cat_accept[\"INCOME_TYPE\"].replace({\"2NDFL\": 0, \"OTHER\": 1}, inplace=True)\n",
        "cat_accept[\"CUSTOMER_CATEGORY_\"].replace({\"Corporate\": 0, \"VIP\": 1}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdtA8IOy1tsX"
      },
      "source": [
        "The following function calculates IV for each varible using WOE. The method of calculating WOE:\n",
        "\n",
        "I separated data in bins, one bin for each category and calculate WOE value for each bin;\n",
        "\n",
        "Then the function calculates IV for each bin and sums up values to get IV for the variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpQn9aBu1tsX"
      },
      "outputs": [],
      "source": [
        "def get_IV_cat(df, feature, target):\n",
        "    lst = []\n",
        "    unique_values = df[feature].unique()\n",
        "    for val in unique_values:\n",
        "        lst.append([feature,                                                                                                                   \n",
        "                    df[(df[feature] == val) & (df[target] == 0)].count()[feature],  # Good \n",
        "                    df[(df[feature] == val) & (df[target] == 1)].count()[feature]   # Bad  \n",
        "                   ])\n",
        "\n",
        " \n",
        "    data = pd.DataFrame(lst, columns=['Variable', 'Good', 'Bad'])\n",
        "\n",
        "        \n",
        "    total_bad = df[df[target] == 1].count()[feature]\n",
        "    total_good = df.shape[0] - total_bad\n",
        "    \n",
        "    data['Distribution Good'] = data['Good']/ total_good\n",
        "    data['Distribution Bad'] = data['Bad'] / total_bad\n",
        "    data['WoE'] = np.log(data['Distribution Good'] / data['Distribution Bad'])\n",
        "    \n",
        "\n",
        "    data = data.replace({'WoE': {np.inf: 0, -np.inf: 0}})\n",
        "\n",
        "    data['IV'] = data['WoE'] * (data['Distribution Good'] - data['Distribution Bad'])\n",
        "    data.index = range(len(data.index))\n",
        "\n",
        "    iv = data['IV'].sum()\n",
        "\n",
        "    return iv\n",
        "    \n",
        "IV = []\n",
        "\n",
        "IV.append(get_IV_cat(cat_accept, 'INCOME_TYPE', 'target'))\n",
        "IV.append(get_IV_cat(cat_accept, 'EDUCATION_', 'target'))\n",
        "IV.append(get_IV_cat(cat_accept, 'CUSTOMER_CATEGORY_', 'target'))\n",
        "IV.append(get_IV_cat(cat_accept, 'DEPENDANT_NUMBER', 'target'))\n",
        "IV.append(get_IV_cat(cat_accept, 'IS_EMPL', 'target'))\n",
        "IV.append(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1v3J20Bb1tsX"
      },
      "outputs": [],
      "source": [
        "data = {'Names': col_names,\n",
        "        'None_values%': percent_non,\n",
        "        'Mode': mode,\n",
        "        'IV': IV\n",
        "       } \n",
        "int_info = pd.DataFrame(data,columns=['Names','None_values%','Mode','IV'])\n",
        "int_info = int_info.drop([5])\n",
        "int_info\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9rJAbUb1tsX"
      },
      "source": [
        "To look if there are any anomaloies I will construct histogram for each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqxhuX061tsX"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 5, figsize=(15,5))\n",
        "\n",
        "axes[0].hist(x = 'INCOME_TYPE', data = cat_accept)\n",
        "axes[0].set_xlabel(\"INCOME_TYPE \")\n",
        "axes[1].hist(x = 'EDUCATION_', data = cat_accept)\n",
        "axes[1].set_xlabel(\"EDUCATION_ \")\n",
        "axes[2].hist(x = 'CUSTOMER_CATEGORY_', data = cat_accept)\n",
        "axes[2].set_xlabel(\"CUSTOMER_CATEGORY_ \")\n",
        "axes[3].hist(x = 'DEPENDANT_NUMBER', data = cat_accept)\n",
        "axes[3].set_xlabel(\"DEPENDANT_NUMBER \")\n",
        "axes[4].hist(x = 'IS_EMPL', data = cat_accept)\n",
        "axes[4].set_xlabel(\"IS_EMPL \")\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bygBRKr91tsY"
      },
      "source": [
        "So, I can say that there is no anomalies. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAYJLrI_1tsY"
      },
      "source": [
        "### 3.2 Reject"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG8vAMN51tsY"
      },
      "source": [
        "Now, I will do the same but for reject.csv. Except for IV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArAWVl1q1tsY"
      },
      "outputs": [],
      "source": [
        "cat_reject1 = pd.DataFrame(df_reject,columns=['INCOME_TYPE','EDUCATION_','CUSTOMER_CATEGORY_',\n",
        "                                             'DEPENDANT_NUMBER', 'IS_EMPL','target'])\n",
        "\n",
        "cat_reject = cat_reject1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9r_jRZLf1tsY"
      },
      "outputs": [],
      "source": [
        "cat_reject['INCOME_TYPE'] = cat_reject['INCOME_TYPE'].fillna(cat_reject.mode()['INCOME_TYPE'][0])\n",
        "cat_reject['EDUCATION_'] = cat_reject['EDUCATION_'].fillna(cat_reject.mode()['EDUCATION_'][0])\n",
        "cat_reject['CUSTOMER_CATEGORY_'] = cat_reject['CUSTOMER_CATEGORY_'].fillna(cat_reject.mode()['CUSTOMER_CATEGORY_'][0])\n",
        "cat_reject['DEPENDANT_NUMBER'] = cat_reject['DEPENDANT_NUMBER'].fillna(cat_reject.mode()['DEPENDANT_NUMBER'][0])\n",
        "cat_reject['IS_EMPL'] = cat_reject['IS_EMPL'].fillna(cat_reject.mode()['IS_EMPL'][0])\n",
        "\n",
        "if(cat_reject.isnull().values.any() == False):\n",
        "    print(\"No more Nans\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HU-zaXrB1tsY"
      },
      "outputs": [],
      "source": [
        "percent_non = []\n",
        "col_names = []\n",
        "\n",
        "\n",
        "for col in cat_reject1:\n",
        "    col_names.append(col)\n",
        "    if(col != 'target'):\n",
        "        percent_non.append((df_reject[col].isnull().sum())/(len(df_reject)/100))\n",
        "    else: \n",
        "        percent_non.append(0)\n",
        "    \n",
        "\n",
        "mode = []\n",
        "\n",
        "for col in cat_reject:\n",
        "    mode.append(cat_reject.mode()[col][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfGSco001tsY"
      },
      "outputs": [],
      "source": [
        "cat_reject2 = cat_reject\n",
        "cat_reject[\"EDUCATION_\"].replace({\"beginner\": 0, \"elementary\": 1, \"high\": 2, \"advanced\": 3}, inplace=True)\n",
        "cat_reject[\"INCOME_TYPE\"].replace({\"2NDFL\": 0, \"OTHER\": 1}, inplace=True)\n",
        "cat_reject[\"CUSTOMER_CATEGORY_\"].replace({\"Corporate\": 0, \"VIP\": 1}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJtYWe2z1tsY"
      },
      "outputs": [],
      "source": [
        "data = {'Names': col_names,\n",
        "        'None_values%': percent_non,\n",
        "        'Mode': mode\n",
        "       } \n",
        "int_info = pd.DataFrame(data,columns=['Names','None_values%','Mode'])\n",
        "int_info = int_info.drop([5])\n",
        "int_info\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QqV5ZvQ1tsY"
      },
      "source": [
        "Checking anomalies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSV7gVNe1tsY"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 5, figsize=(15,5))\n",
        "\n",
        "axes[0].hist(x = 'INCOME_TYPE', data = cat_reject)\n",
        "axes[0].set_xlabel(\"INCOME_TYPE \")\n",
        "axes[1].hist(x = 'EDUCATION_', data = cat_reject)\n",
        "axes[1].set_xlabel(\"EDUCATION_ \")\n",
        "axes[2].hist(x = 'CUSTOMER_CATEGORY_', data = cat_reject)\n",
        "axes[2].set_xlabel(\"CUSTOMER_CATEGORY_ \")\n",
        "axes[3].hist(x = 'DEPENDANT_NUMBER', data = cat_reject)\n",
        "axes[3].set_xlabel(\"DEPENDANT_NUMBER \")\n",
        "axes[4].hist(x = 'IS_EMPL', data = cat_reject)\n",
        "axes[4].set_xlabel(\"IS_EMPL \")\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji96PH6a1tsY"
      },
      "source": [
        "No anomalies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pzTenWV1tsY"
      },
      "source": [
        "## 4. Построить логистическую регрессию только на одобренных заявках с преобразованными переменными WoE. Какое значение GINI? F1 мера?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghS05cfM1tsY"
      },
      "source": [
        "Firstly I will create WOE variables for each variable, using the same functions as before with small corrections in order to calculate columns for the new WOE variables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXsod4MJ1tsY"
      },
      "outputs": [],
      "source": [
        "all_accept = pd.concat([int_accept, cat_accept], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-JKCPQO1tsY"
      },
      "outputs": [],
      "source": [
        "def get_IV(df, feature, target):\n",
        "    lst = []\n",
        "    bins = df[feature].quantile(np.linspace(0, 1, 21)).values\n",
        "    bin_index = range(1, len(bins))\n",
        "    for index in bin_index:\n",
        "        val = df[(df[feature] >= bins[index-1])&(df[feature] <= bins[index])]\n",
        "        bin_low = bins[index-1]\n",
        "        bin_high = bins[index]\n",
        "        lst.append([feature,\n",
        "                    bin_low,\n",
        "                    bin_high,\n",
        "                    val[(val[target] == 0)].count()[feature],  # Good \n",
        "                    val[(val[target] == 1)].count()[feature]   # Bad  \n",
        "                   ])\n",
        " \n",
        "    data = pd.DataFrame(lst, columns=['Variable', 'Low', 'High', 'Good', 'Bad'])\n",
        "\n",
        "        \n",
        "    total_bad = df[df[target] == 1].count()[feature]\n",
        "    total_good = df.shape[0] - total_bad\n",
        "    \n",
        "    data['Distribution Good'] = data['Good']/ total_good\n",
        "    data['Distribution Bad'] = data['Bad'] / total_bad\n",
        "    data['WoE'] = np.log(data['Distribution Good'] / data['Distribution Bad'])\n",
        "\n",
        "    data = data.replace({'WoE': {np.inf: 0, -np.inf: 0}})\n",
        "    data = data.sort_values(by=['WoE'], ascending=[True])\n",
        "    data.index = range(len(data.index))\n",
        "    \n",
        "    data1 = data\n",
        "    \n",
        "        \n",
        "    #Combine bins with similar WOE values\n",
        "    n = range(1, len(data1))\n",
        "    for i in n:\n",
        "        if (data1['WoE'][i] - data1['WoE'][i-1] < 0.08):\n",
        "            data.loc[i,'WoE'] = data1['WoE'][i-1]\n",
        "\n",
        "    data = data.sort_values(by=['Low'], ascending=[True])\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRCCP2s41tsY"
      },
      "outputs": [],
      "source": [
        "def get_IV_cat(df, feature, target):\n",
        "    lst = []\n",
        "    unique_values = df[feature].unique()\n",
        "    for val in unique_values:\n",
        "        bin_val = val\n",
        "        lst.append([feature, \n",
        "                    val,\n",
        "                    df[(df[feature] == val) & (df[target] == 0)].count()[feature],  # Good \n",
        "                    df[(df[feature] == val) & (df[target] == 1)].count()[feature]   # Bad  \n",
        "                   ])\n",
        "\n",
        " \n",
        "    data = pd.DataFrame(lst, columns=['Variable','Bin_val' ,'Good', 'Bad'])\n",
        "\n",
        "        \n",
        "    total_bad = df[df[target] == 1].count()[feature]\n",
        "    total_good = df.shape[0] - total_bad\n",
        "    \n",
        "    data['Distribution Good'] = data['Good']/ total_good\n",
        "    data['Distribution Bad'] = data['Bad'] / total_bad\n",
        "    data['WoE'] = np.log(data['Distribution Good'] / data['Distribution Bad'])\n",
        "    \n",
        "\n",
        "    data = data.replace({'WoE': {np.inf: 0, -np.inf: 0}})\n",
        "\n",
        "    data['IV'] = data['WoE'] * (data['Distribution Good'] - data['Distribution Bad'])\n",
        "    data = data.sort_values(by=['WoE'], ascending=[True])\n",
        "    data.index = range(len(data.index))\n",
        "\n",
        "    iv = data['IV'].sum()\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwW2zmGE1tsZ"
      },
      "outputs": [],
      "source": [
        "def make_woe_cat(df, feature, target):\n",
        "    data = get_IV_cat(df, feature, target)\n",
        "    for i in range(0, len(data)):      \n",
        "        df.loc[(df[feature] == data['Bin_val'][i]), feature +'WOE'] = data['WoE'][i] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pBWRLJi1tsZ"
      },
      "outputs": [],
      "source": [
        "make_woe_cat(all_accept, 'INCOME_TYPE', 'target')\n",
        "make_woe_cat(all_accept, 'EDUCATION_', 'target')\n",
        "make_woe_cat(all_accept, 'CUSTOMER_CATEGORY_', 'target')\n",
        "make_woe_cat(all_accept, 'DEPENDANT_NUMBER', 'target')\n",
        "make_woe_cat(all_accept, 'IS_EMPL', 'target')\n",
        "\n",
        "for col in int_accept:\n",
        "    data = get_IV(all_accept, col, 'target')\n",
        "    for i in range(0, len(data)): \n",
        "        all_accept.loc[(all_accept[col] >= data['Low'][i])&(all_accept[col] <= data['High'][i]), col +'WOE'] = data['WoE'][i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wAln1Nl1tsZ"
      },
      "outputs": [],
      "source": [
        "all_accept.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7tXHb9r1tsZ"
      },
      "source": [
        "Now, I will build logistic regression on obtained WOE variables:\n",
        "\n",
        "I will split data set into a training set and a validation set and then fit a logistic regression model using only the training observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YsoFhKm1tsZ"
      },
      "outputs": [],
      "source": [
        "X = all_accept[['INCOME_TYPEWOE', 'EDUCATION_WOE', 'CUSTOMER_CATEGORY_WOE', 'DEPENDANT_NUMBERWOE', 'IS_EMPLWOE',\n",
        "               'incomeWOE', 'COUNT_ACTIVE_CREDIT_NO_CCWOE', 'CNT_MNTH_FROM_LAST_PMNTWOE', 'ageWOE', 'experienceWOE',\n",
        "               'inquiry_14_dayWOE', 'inquiry_21_dayWOE', 'count_mnth_act_passportWOE']]\n",
        "\n",
        "y = all_accept['target']\n",
        "X_train, X_test, y_train, y_test = tts(X, y, test_size = 0.33, random_state=0)\n",
        "\n",
        "logreg_accept = LogisticRegression()\n",
        "logreg_accept.fit(X_train, y_train)\n",
        "\n",
        "logit1 = sm.Logit(y,sm.add_constant(X)).fit()\n",
        "\n",
        "roc = 0.5\n",
        "thresh = 0.5\n",
        "\n",
        "for t in np.linspace(0, 1, 1000):\n",
        "    predict = np.where(logreg_accept.predict_proba(X_train)[:,1] > t, 1, 0) #prob of 1\n",
        "    roc_t = roc_auc_score(y_train, predict)\n",
        "    if roc_t > roc:\n",
        "        roc = roc_t\n",
        "        best_thresh = t\n",
        "        \n",
        "predict = np.where(logreg_accept.predict_proba(X_test)[:,1] > best_thresh, 1, 0)\n",
        "print(\"GINI:\", 2*roc_auc_score(y_test, predict) - 1)\n",
        "print(\"F1 Score:\", f1_score(y_test, predict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4A2_o0w1tsZ"
      },
      "source": [
        "## 5. Провести анализ Reject Inference. Какая доля отказанных заявок? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHJduy5l1tsZ"
      },
      "source": [
        "Firstly, I will create WOE variables foe each variable for the reject.csv by using WOE from accept.csv. For the values from reject.csv that are larger than the max bin or lower than the min bin from accept.csv I will give WOE of the highest and the lowest bins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnybPlgp1tsZ"
      },
      "outputs": [],
      "source": [
        "all_reject = pd.concat([int_reject, cat_reject], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg0eYBEJ1tsZ"
      },
      "outputs": [],
      "source": [
        "def make_woe_cat_r(df_acc, df_rej, feature, target):\n",
        "    data = get_IV_cat(df_acc, feature, target)\n",
        "    for i in range(0, len(data)):      \n",
        "        df_rej.loc[(df_rej[feature] == data['Bin_val'][i]), feature +'WOE'] = data['WoE'][i] \n",
        "\n",
        "make_woe_cat_r(all_accept, all_reject, 'INCOME_TYPE', 'target')\n",
        "make_woe_cat_r(all_accept, all_reject, 'EDUCATION_', 'target')\n",
        "make_woe_cat_r(all_accept, all_reject, 'CUSTOMER_CATEGORY_', 'target')\n",
        "make_woe_cat_r(all_accept, all_reject, 'DEPENDANT_NUMBER', 'target')\n",
        "make_woe_cat_r(all_accept, all_reject, 'IS_EMPL', 'target')\n",
        "\n",
        "for col in int_reject:\n",
        "    data = get_IV(all_accept, col, 'target')\n",
        "    data.loc[0,'Low'] = all_reject[col].min() \n",
        "    data.loc[19,'High'] = all_reject[col].max() \n",
        "    for i in range(0, len(data)): \n",
        "        all_reject.loc[(all_reject[col] >= data['Low'][i])&(all_reject[col] <= data['High'][i]), col +'WOE'] = data['WoE'][i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2I96_Brl1tsZ"
      },
      "outputs": [],
      "source": [
        "all_reject.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "700Xlc7X1tsZ"
      },
      "source": [
        "Now, I will use my logistic regression model from accept.csv in order to find predictions for 'target' for reject.csv:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sT_BGBP01tsZ"
      },
      "outputs": [],
      "source": [
        "Xr = all_reject[['INCOME_TYPEWOE', 'EDUCATION_WOE', 'CUSTOMER_CATEGORY_WOE', 'DEPENDANT_NUMBERWOE', 'IS_EMPLWOE',\n",
        "               'incomeWOE', 'COUNT_ACTIVE_CREDIT_NO_CCWOE', 'CNT_MNTH_FROM_LAST_PMNTWOE', 'ageWOE', 'experienceWOE',\n",
        "               'inquiry_14_dayWOE', 'inquiry_21_dayWOE', 'count_mnth_act_passportWOE']]\n",
        "\n",
        "        \n",
        "predict = np.where(logreg_accept.predict_proba(Xr)[:,1] > best_thresh, 1, 0)\n",
        "all_reject['target'] = predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_UZAs401tsZ"
      },
      "source": [
        "Calculate share of rejected applications:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlQL3Emy1tsZ"
      },
      "outputs": [],
      "source": [
        "len(all_reject) / (len(all_reject) + len(all_accept))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXVfTl541tsZ"
      },
      "source": [
        "## 6. Построить логистическую регрессию на всех заявках с преобразованными переменными WoE. Какое значение GINI, F1? Изменилась ли модель?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MvHGeNs1tsZ"
      },
      "source": [
        "Combine two tables:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSOBcT1v1tsZ"
      },
      "outputs": [],
      "source": [
        "all_app = pd.concat([all_accept, all_reject]).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx7iq_ZW1tsZ"
      },
      "source": [
        "Now, build logistic regression model on all_app and calculate GINI and F1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdMmCrDw1tsZ"
      },
      "outputs": [],
      "source": [
        "X = all_app[['INCOME_TYPEWOE', 'EDUCATION_WOE', 'CUSTOMER_CATEGORY_WOE', 'DEPENDANT_NUMBERWOE', 'IS_EMPLWOE',\n",
        "               'incomeWOE', 'COUNT_ACTIVE_CREDIT_NO_CCWOE', 'CNT_MNTH_FROM_LAST_PMNTWOE', 'ageWOE', 'experienceWOE',\n",
        "               'inquiry_14_dayWOE', 'inquiry_21_dayWOE', 'count_mnth_act_passportWOE']]\n",
        "\n",
        "y = all_app['target']\n",
        "X_train, X_test, y_train, y_test = tts(X, y, test_size = 0.33, random_state=0)\n",
        "\n",
        "logreg_accept = LogisticRegression()\n",
        "logreg_accept.fit(X_train, y_train)\n",
        "\n",
        "logit2 = sm.Logit(y,sm.add_constant(X)).fit()\n",
        "\n",
        "roc = 0.5\n",
        "thresh = 0.5\n",
        "\n",
        "for t in np.linspace(0, 1, 1000):\n",
        "    predict = np.where(logreg_accept.predict_proba(X_train)[:,1] > t, 1, 0) #prob of 1\n",
        "    roc_t = roc_auc_score(y_train, predict)\n",
        "    if roc_t > roc:\n",
        "        roc = roc_t\n",
        "        best_thresh = t\n",
        "        \n",
        "predict = np.where(logreg_accept.predict_proba(X_test)[:,1] > best_thresh, 1, 0)\n",
        "print(\"GINI:\", 2*roc_auc_score(y_test, predict) - 1)\n",
        "print(\"F1 Score:\", f1_score(y_test, predict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u09vDN5K1tsa"
      },
      "source": [
        "We can see that both GINI coefficient and F1 score increased => the model with all applictions has a better performace. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JS8uhzdM1tsa"
      },
      "outputs": [],
      "source": [
        "print(\"1st Model\", logit1.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8N4UXiF1tsa"
      },
      "outputs": [],
      "source": [
        "print(\"2nd Model\", logit2.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_HzuwG-1tsa"
      },
      "source": [
        "We can see that $R^2$ statistic is also higher for the second model. And we can observe changes in the sign of some coefficients and also change in the significance of some variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbje2ej21tsa"
      },
      "source": [
        "## 7. Какую модель вы рекомендуете для внедрения в продуктивную среду?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ujvc9c01tsa"
      },
      "source": [
        "   First of all we can see that such measures of a model's accuracy as GINI coefficient, F1 score and R-squared are all higher for the second model. Moreover, the seconed model was trained on the larger amount of data, which means that it increases the chance of overfitting and has more balanced classes. \n",
        "\n",
        "   However, the second model can be risky: if the model misclassified a large amount of \"bad\" clients as \"good\", this would have a negative impact on the accuracy of predictions and using this model may lead to giving credits to undesirable clients who will have debt problems. On the other hand, as we cannot say exactly if they were classified correctly or not, there can be a situation where we lost potential clients and they were classified to reject.csv and using reject inference helped to find such clients and therefore adding more \"good\" clients to the training set increases the accuracy. \n",
        "   \n",
        "   Overall, in my example I would recommend to use the second model, due to the reason that first model has very low measures of a model's accuracy. In general, I think that model built by second method will also be preferable, but for a better performance, it would be helpful to decrease the risk of misclassification for the reject.csv clients, for example by adding restrictions on what clients are definitely undesirable. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RabFZGD91tsa"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "SAS_Project_2.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}